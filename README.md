# JobSearch CLI + Data Pipeline
A Python-powered job market data extraction, cleaning, and analysis toolkit.

## Overview
JobSearch is a full-featured data pipeline that integrates with the SerpAPI Google Jobs API to:
- Search job listings across any U.S. state or all 50 states
- Save job postings as CSV datasets
- Combine, filter, and clean datasets using a dedicated processing engine
- Parse salaries into structured and annualized values
- Analyze job descriptions for keyword frequencies
- Calculate salary statistics across states or time periods
- Use a professional, multi-command CLI interface
  
This project turns raw job market data into actionable intelligence for researchers, analysts, and job seekers.

## Features

### Job Search
- Query job listings by state or all 50 states
- Automatic pagination handling
- Retry logic for API processing delays
- Optional CSV output

### Dataset Creation
- Combine multiple saved CSVs into unified datasets
- Filter by:
  - Job title
  - State or all states
  - Specific date (YYYY-MM-DD)
  - Year, month, or day

### Keyword Analysis
- Search job descriptions for any number of keywords
- See:
  - Total mentions
  - Occurrences per job listing
  - Percentage of total listings
  - Percentage of filtered subset

### Salary Parsing & Statistics
- Converts salaries like:
  - 130K–160K a year
  - $30–37.50/hr
  - US$6,211–15,211/month
- Into:
  - Min, max, average
  - Annualized salary
  - Statistical summaries

### Command-Line Interface
A fully structured CLI with subcommands:
```
search
search_all
create_dataset
filter
sstats
```

# Setup
1. Install required dependencies:
```bash
pip install pandas tqdm requests
```
2. Install via pip
```bash
pip install jobsearch-cli
```
3. Obtain a SerpAPI key
Create one here:
Sign up at: https://serpapi.com/

# Package Structure 
```bash
jobsearch/                 # PACKAGE (importable)
│
├── __init__.py            # Makes this directory a package
├── jobsearch.py           # Searching engine, salary parsing, dataset logic
├── cli.py                 # Command-line interface entry point
│
~/.jobsearch/Job_Listings/ # User dataset storage (auto-created)
│
pyproject.toml             # Build + packaging configuration
README.md
LICENSE
```
### Where datasets are stored
All CSV files generated by the CLI or API searches are saved automatically to:
```
C:/Users/<you>/.jobsearch/Job_Listings/
```

# How Users Can Use the Package

### Import programmatically
```python
from jobsearch import JobSearch, Clean

# Example for Fetching data from API
j = JobSearch("YOUR_API_KEY")
df = j.search_all_states("Cybersecurity", save=True)

# Example for loading and analyzing saved data (Must have CSVs to filter through)
c = Clean()
dataset = c.create_dataset("Cybersecurity", all_states=True)
filter = c.filterdesc(dataset, "python", "aws", "splunk", "COMPTIA")
salary_stats = c.salary_stats(dataset)

print(filter)
print(salary_stats)
```
Example Output:

<img width="781" height="118" alt="Capture 3" src="https://github.com/user-attachments/assets/5be96347-03ad-4df9-9ce1-3a7587ae83ce" />

### Use the CLI after installation
Note: You must have CSVs downloaded for filter and sstats functionality.
Search a single state
```bash
jobsearch search --state "California" --job "Developer"
```
Search all 50 states
```bash
jobsearch search_all --job "Cybersecurity"
```
Filter job descriptions 
```bash
jobsearch filter --keywords python aws --all --job Analyst
```
View salary statistics
```bash
jobsearch sstats --state "Texas" --job "Software Engineer"
```


